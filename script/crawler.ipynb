{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Aufbau der Seite \n",
    "\n",
    "Fachgebiete\n",
    "    https://arxiv.org/ <br>\n",
    "    /archive/<?:str>\n",
    "\n",
    "    Jahr\n",
    "        /year/<astro-ph:str>/<number:int>\n",
    "        https://arxiv.org/list/<astro-ph:str>/<2301:int>\n",
    "\n",
    "        Paper pages\n",
    "            e.g.: <br>\n",
    "            https://arxiv.org/list/astro-ph/2301?skip=25&show=25\n",
    "\n",
    "        Abstract:\n",
    "            https://arxiv.org/abs/2301.11140\n",
    "\n",
    "\n",
    "    \n",
    "    Methodik\n",
    "    request arxiv.org \n",
    "    parse themes\n",
    "\n",
    "        request html\n",
    "        parse year\n",
    "        for year in years:\n",
    "            request\n",
    "            parse month\n",
    "            for month in month:\n",
    "                request\n",
    "                parse years\n",
    "                for page in pages\n",
    "                    request\n",
    "                    parse paper\n",
    "                    for paper in papers\n",
    "                        request\n",
    "                        parse paper\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from database import Database\n",
    "import configuration\n",
    "import datetime\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# create sqlite3 database\n",
    "path = \"./../data/\"                                     # change if needed\n",
    "#db_name = \"cs.db\"                                      # change if needed\n",
    "db_name = configuration.DATABASE_NAME                   # change if needed\n",
    "\n",
    "\n",
    "# empty for crawling all\n",
    "# for white_list and ignore_list watch below (cell 4)\n",
    "only_themes_to_crawl:list[str] = []                     # change if needed \n",
    "\n",
    "db = \"{}{}\".format(path, db_name)\n",
    "print(db)\n",
    "\n",
    "# to create new database\n",
    "#Database().createDatabase(db)                          # comment out if new db needed\n",
    "\n",
    "conn = sqlite3.connect(db)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('SELECT COUNT(*) FROM paper')\n",
    "count = cursor.fetchone()[0]\n",
    "print(\"Number of rows in 'paper':\", count)\n",
    "\n",
    "domain = \"https://arxiv.org/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = requests.get(domain).content\n",
    "document = (str) (document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = []\n",
    "\n",
    "positions = []\n",
    "positions=([m.end() for m in re.finditer(\"/archive/\",document)])\n",
    "\n",
    "for start in positions:\n",
    "    end = document.find('\"', start)\n",
    "    themes.append((document[start:end]))\n",
    "\n",
    "# cs (computer science) is a special case does not get catched by choosen method\n",
    "themes.append(\"cs\")\n",
    "\n",
    "print(themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use numbers as string\n",
    "black_list = [\n",
    "        {\n",
    "            'theme' : '',\n",
    "            'ignore_theme' : False,\n",
    "            'year' : [],\n",
    "            'month' : []\n",
    "        }\n",
    "]\n",
    "\n",
    "white_list = [\n",
    "        {\n",
    "            'theme' : '',\n",
    "            'year' : [],\n",
    "            'month' : []\n",
    "        }\n",
    "]\n",
    "\n",
    "if len(only_themes_to_crawl) > 0:\n",
    "    themes = only_themes_to_crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Black listed:\")\n",
    "print(black_list)\n",
    "\n",
    "print(\"\\n\\nWhite listed:\")\n",
    "print(white_list)\n",
    "print()\n",
    "print(themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Starting crawling: {}\".format(themes))\n",
    "for theme in themes:\n",
    "    skip_theme = False\n",
    "    for black_entry in black_list:\n",
    "        if theme == black_entry['theme'] and black_entry['ignore_theme'] == True:\n",
    "            skip_theme = True\n",
    "            break\n",
    "    for white_entry in white_list:\n",
    "        if theme == white_entry['theme']:# and white_entry['ignore_theme'] == True:\n",
    "            skip_theme = False\n",
    "            break\n",
    "    if skip_theme == True:\n",
    "        skip_theme = False\n",
    "        continue\n",
    "    print(\"Starting crawling: {}\".format(theme))\n",
    "\n",
    "    url_theme = domain + \"archive/\" + theme\n",
    "    print(url_theme)\n",
    "    document = requests.get(url_theme).content\n",
    "    document = (str) (document)\n",
    "    \n",
    "    url_year = \"year/\" + theme + \"/\"\n",
    "    positions = []\n",
    "    positions=([m.end() for m in re.finditer(url_year,document)])\n",
    "    print(url_year)\n",
    "    \n",
    "    years = []\n",
    "        \n",
    "    for start in positions:\n",
    "        end = document.find('\"', start)\n",
    "        years.append((document[start:end]))\n",
    "    print(\"Found years: {}\".format(years))\n",
    "    print()\n",
    "\n",
    "\n",
    "    for year in years:\n",
    "        \n",
    "        skip_year = False\n",
    "        for black_entry in black_list:\n",
    "            if theme == black_entry['theme'] and year in black_entry['year']:\n",
    "                skip_year = True\n",
    "                break\n",
    "        for white_entry in white_list:\n",
    "            if theme == white_entry['theme'] and year in white_entry['year']:\n",
    "                skip_year = False\n",
    "                break\n",
    "        if skip_year == True:\n",
    "            skip_year = False\n",
    "            continue\n",
    "        print(\"Starting crawling: {}\".format(year))\n",
    "\n",
    "\n",
    "        url_month = domain + url_year + year\n",
    "        print(url_month)\n",
    "        document = requests.get(url_month).content\n",
    "        document = (str) (document)\n",
    "        \n",
    "        url_month = \"/list/\" + theme + \"/\"\n",
    "        print(url_month)\n",
    "        positions = []\n",
    "        positions=([m.end() for m in re.finditer(url_month,document)])\n",
    "        \n",
    "        months = []\n",
    "            \n",
    "        for start in positions:\n",
    "            end = document.find('\"', start)\n",
    "            value = document[start:end]\n",
    "            if not value.isdigit():\n",
    "                continue\n",
    "            if value in months:\n",
    "                continue\n",
    "            months.append((value))\n",
    "        print(\"Found month in year {}: {}\".format(year, months))\n",
    "        \n",
    "        \n",
    "        findString = \"<small>[ total of \"\n",
    "        for month in months:\n",
    "            \n",
    "            skip_month = False\n",
    "            for black_entry in black_list:\n",
    "                if theme == black_entry['theme'] and month in black_entry['month']:\n",
    "                    skip_month = True\n",
    "                    break\n",
    "            for white_entry in white_list:\n",
    "                if theme == white_entry['theme'] and month in white_entry['month']:\n",
    "                    skip_month = False\n",
    "                    break\n",
    "            if skip_month == True:\n",
    "                skip_month = False\n",
    "                continue\n",
    "            print(\"Starting crawling: {}\".format(month))\n",
    "\n",
    "            \n",
    "            url_month = domain + \"list/\" + theme + \"/\" + month\n",
    "            print(url_month)\n",
    "            document = requests.get(url_month).content\n",
    "            document = (str) (document)\n",
    "            filter = \"<small>[ total of \"\n",
    "\n",
    "            start_position = document.find(filter)\n",
    "            start_position = start_position + len(filter)\n",
    "\n",
    "            end_position = document.find(\"entries\",start_position)\n",
    "            number_of_papers = (int)(document[start_position:end_position])\n",
    "            print(\"number of papers: {}\".format(number_of_papers))\n",
    "            #print(url_month)\n",
    " \n",
    "            number_of_papers_per_page = 100\n",
    "            for page in range(0, number_of_papers, number_of_papers_per_page):\n",
    "                url_page = url_month + \"?skip={}&show={}\".format(page,number_of_papers_per_page)\n",
    "                #print(url_page)\n",
    "                document = requests.get(url_page).content\n",
    "                document = (str) (document)\n",
    "                filter = '<a href=\"/abs/'\n",
    "                papers_url = domain + \"abs/\"\n",
    "                positions = []\n",
    "                positions=([m.end() for m in re.finditer(filter,document)])\n",
    "                papers = []\n",
    "                    \n",
    "                for start in positions:\n",
    "                    end = document.find('\"', start)\n",
    "                    value = document[start:end]\n",
    "                    \n",
    "                    papers.append((value))\n",
    "\n",
    "                #print(papers)\n",
    "\n",
    "\n",
    "                url = domain + \"abs/\"\n",
    "                for paper in papers:\n",
    "                    paper_url = url + paper\n",
    "                    #print(paper_url)\n",
    "                    cursor.execute('SELECT COUNT(*) FROM paper WHERE url=?', (paper_url,))\n",
    "                    result = cursor.fetchone()\n",
    "\n",
    "                    if result[0] == 0:\n",
    "                        document = requests.get(paper_url).content\n",
    "                        document = (str) (document)\n",
    "\n",
    "                        cursor.execute(\"INSERT INTO paper (url, theme, timestamp, content) VALUES (?, ?, ?, ?)\", (paper_url, theme, datetime.datetime.now(), document))\n",
    "                        conn.commit()\n",
    "                        #print(paper_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('SELECT COUNT(*) FROM paper')\n",
    "count = cursor.fetchone()[0]\n",
    "print(\"Number of rows in 'paper':\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('SELECT * FROM paper')\n",
    "document = cursor.fetchall()[0]\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
